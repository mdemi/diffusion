{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from Diffusion import *\n",
    "from utils import makedir\n",
    "import torch\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import h5py as h5\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "local_storage_dir = \"/home/md775/LocalStorage/MLProjects/Diffusion/\" # Change this to your storage directory\n",
    "dataset_path = local_storage_dir + \"Datasets/IntensityFrontier/if-image-train.h5\"\n",
    "checkpoint_dir = local_storage_dir + \"Checkpoints/IntensityFrontier/\"\n",
    "log_dir = local_storage_dir + \"Logs/\"\n",
    "gif_dir = os.getcwd() + \"/gifs/\"\n",
    "makedir(checkpoint_dir)\n",
    "makedir(log_dir)\n",
    "makedir(gif_dir)\n",
    "num_channels = 1 # 1 for grayscale\n",
    "num_timesteps = 2000 # Number of timesteps of the diffusion process\n",
    "beta_min = 1e-6\n",
    "beta_max = 0.99\n",
    "image_size = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "class hdf5Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, load_all=False, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset_path = dataset_path\n",
    "        self.load_all = load_all\n",
    "        if load_all:\n",
    "            self.dataset = h5.File(dataset_path, 'r')['image'][:]\n",
    "            self.dataset = self.dataset[:,None,:,:]\n",
    "            self.dataset = torch.from_numpy(self.dataset).float()\n",
    "        else:\n",
    "            self.dataset = h5.File(dataset_path, 'r')['image']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.load_all:\n",
    "            image = self.dataset[idx]\n",
    "        else:\n",
    "            image = self.dataset[idx][None,:,:]\n",
    "            image = torch.from_numpy(image).float()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image        \n",
    "\n",
    "transform = transforms.Resize((image_size, image_size), antialias=True)\n",
    "\n",
    "image_dataset = hdf5Dataset(dataset_path, load_all=False, transform=transform)\n",
    "dataloader = DataLoader(image_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=16, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diffusion model\n",
    "DiffusionModel = Diffusion(\n",
    "    image_size=image_size,\n",
    "    num_channels = num_channels,\n",
    "    num_timesteps=num_timesteps,\n",
    "    beta_min=beta_min,\n",
    "    beta_max=beta_max,\n",
    "    beta_schedule=\"cosine\",\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forward process\n",
    "def image_from_tensor(tensor):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t.cpu().numpy().astype(np.float32)),\n",
    "    ])\n",
    "    return reverse_transforms(tensor)\n",
    "\n",
    "initial_tensor = next(iter(dataloader)).to(device)\n",
    "plt.imshow(image_from_tensor(initial_tensor[0]), cmap='gray')\n",
    "\n",
    "plt.figure(figsize=(30,60))\n",
    "num_images = 10\n",
    "stepsize = int(num_timesteps/num_images)\n",
    "for idx in range(0, num_timesteps, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(int(num_images+1/8)+1, 8, int(idx/stepsize) + 1)\n",
    "    tensor, noise = DiffusionModel.forward_process(initial_tensor[0,None], t)\n",
    "    plt.imshow(image_from_tensor(tensor[0]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Unet model\n",
    "model = DiffusionModel.create_model(\n",
    "    num_init_ch=64,\n",
    "    num_downsamples=5,\n",
    "    num_mid_convs=1\n",
    "    )\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def loss_fn(true,pred):\n",
    "    return F.mse_loss(true, pred) + F.l1_loss(true, pred)\n",
    "\n",
    "load_from_checkpoint = False\n",
    "if load_from_checkpoint:\n",
    "    DiffusionModel.load_from_checkpoint(checkpoint_dir+\"model_min_loss.pt\", model, optimizer, lr_scheduler)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "DiffusionModel.train_model(\n",
    "    epochs=40,\n",
    "    data_loader=dataloader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    loss_function=loss_fn,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    checkpoint_interval=1,\n",
    "    log_dir=log_dir+timestamp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from model\n",
    "DiffusionModel.load_from_checkpoint(checkpoint_dir+\"model_min_loss.pt\", model)\n",
    "tensor_sample = DiffusionModel.sample(num_images=1, variance_coeff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reverse process\n",
    "plt.figure(figsize=(60,60))\n",
    "num_images = 10\n",
    "stepsize = int(num_timesteps/num_images)\n",
    "for i, tensor in enumerate(tensor_sample[0]):\n",
    "    if i % stepsize == 0:\n",
    "        plt.subplot(int(num_images+1/8)+1, 8, int(i/stepsize) + 1)\n",
    "        plt.imshow(image_from_tensor(tensor), cmap='gray')\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image_from_tensor(tensor_sample[-1][-1]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gif\n",
    "from PIL import Image\n",
    "images = []\n",
    "num_images = 200\n",
    "stepsize = int(num_timesteps/num_images)\n",
    "for t,tensor in enumerate(tensor_sample):\n",
    "    if t % stepsize == 0:\n",
    "        image_array = image_from_tensor(tensor).astype(np.uint8)\n",
    "        image = Image.fromarray(image_array).convert('RGB')\n",
    "        images.append(image)\n",
    "images[0].save(os.getcwd() + '/samples/particles.gif', save_all=True, append_images=images[1:], duration=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
